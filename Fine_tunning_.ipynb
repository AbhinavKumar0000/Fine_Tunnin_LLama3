{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPljG1xOHt6t0lOgmZAEwLa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a69699b0d30349fc8c1255139b2fdb68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fe63e9c1d554095a4087fe23412a3d7",
              "IPY_MODEL_55ab31324fc6487380990aed74dbf46f",
              "IPY_MODEL_b734a4dcbcce49d8b476306bdb0ae7ca"
            ],
            "layout": "IPY_MODEL_c169b9c083524a839b9bddc5a9b75357"
          }
        },
        "9fe63e9c1d554095a4087fe23412a3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83b05933c9514ca39d081227b841e4fa",
            "placeholder": "​",
            "style": "IPY_MODEL_a371bfa4ae5c440da8e7035ade5908d1",
            "value": "Map: 100%"
          }
        },
        "55ab31324fc6487380990aed74dbf46f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab9485454112442bbd7f2da8005792e5",
            "max": 10178,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9d8d8c5f2fb4bfaa663b6e2d4813697",
            "value": 10178
          }
        },
        "b734a4dcbcce49d8b476306bdb0ae7ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20a0969956054248a6d8fb9d23530374",
            "placeholder": "​",
            "style": "IPY_MODEL_32f7988edc3d47e393ee8e466ae29771",
            "value": " 10178/10178 [00:01&lt;00:00, 7167.35 examples/s]"
          }
        },
        "c169b9c083524a839b9bddc5a9b75357": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83b05933c9514ca39d081227b841e4fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a371bfa4ae5c440da8e7035ade5908d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab9485454112442bbd7f2da8005792e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9d8d8c5f2fb4bfaa663b6e2d4813697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20a0969956054248a6d8fb9d23530374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32f7988edc3d47e393ee8e466ae29771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "433e5b75617e450cbffe5016f35ea4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03e960d28a314b508d2641e20f837134",
              "IPY_MODEL_0a375ccacab3429d82fb94d4d04ae5d1",
              "IPY_MODEL_4e2df339b755498cb4fd7cee3d3295ee"
            ],
            "layout": "IPY_MODEL_7db5ea275d4b495dab6a61124bd21aae"
          }
        },
        "03e960d28a314b508d2641e20f837134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23aa20b9b89447b186c320194f558ee4",
            "placeholder": "​",
            "style": "IPY_MODEL_5f80908b7a2047f493f5be364af61e8e",
            "value": "Unsloth: Tokenizing [&quot;text&quot;]: 100%"
          }
        },
        "0a375ccacab3429d82fb94d4d04ae5d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5edc5ae66ce0474d8e6553ec748a4848",
            "max": 10178,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83fd4714ebab41dcaf667e405c083e32",
            "value": 10178
          }
        },
        "4e2df339b755498cb4fd7cee3d3295ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5758e2c3895c4b239a1f5ed7cf0eacae",
            "placeholder": "​",
            "style": "IPY_MODEL_d5f6ffb467ff460ba360186a62372ca9",
            "value": " 10178/10178 [00:23&lt;00:00, 447.49 examples/s]"
          }
        },
        "7db5ea275d4b495dab6a61124bd21aae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23aa20b9b89447b186c320194f558ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f80908b7a2047f493f5be364af61e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5edc5ae66ce0474d8e6553ec748a4848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83fd4714ebab41dcaf667e405c083e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5758e2c3895c4b239a1f5ed7cf0eacae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5f6ffb467ff460ba360186a62372ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhinavKumar0000/Fine_Tunnin_LLama3/blob/main/Fine_tunning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"HPAI-BSC/medqa-cot-llama31\")"
      ],
      "metadata": {
        "id": "Ay7-z5X7sVJa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UIF8fJVMxhFN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! Llama 3 is up to 8k\n",
        "dtype = None\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-it-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-it-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "]\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\", # Llama-3 70b also works (just change the model name)\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSGOQ9XPvcS3",
        "outputId": "580d742a-3f35-49ee-bb48-85c81c3ccde9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.7.9: Fast Llama patching. Transformers: 4.53.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "IJuDkQF0wGQN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alpaca-style system prompt\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Make sure tokenizer is defined\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    # The MedQA-CoT dataset has these fields:\n",
        "    # - system_prompt (fixed for all examples)\n",
        "    # - question (instruction)\n",
        "    # - response (output)\n",
        "    # Input field is empty since questions are self-contained\n",
        "\n",
        "    instructions = examples[\"question\"]\n",
        "    # Since there's no explicit input context, we'll use empty string\n",
        "    inputs = [\"\"] * len(instructions)\n",
        "    outputs = examples[\"response\"]\n",
        "\n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        # Use the specific system_prompt from the dataset for each example\n",
        "        full_instruction = f\"{examples['system_prompt'][0]}\\n\\n{instruction}\"\n",
        "        text = alpaca_prompt.format(full_instruction, input_text, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"HPAI-BSC/medqa-cot-llama31\", split=\"train\")\n",
        "\n",
        "# Apply formatting\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a69699b0d30349fc8c1255139b2fdb68",
            "9fe63e9c1d554095a4087fe23412a3d7",
            "55ab31324fc6487380990aed74dbf46f",
            "b734a4dcbcce49d8b476306bdb0ae7ca",
            "c169b9c083524a839b9bddc5a9b75357",
            "83b05933c9514ca39d081227b841e4fa",
            "a371bfa4ae5c440da8e7035ade5908d1",
            "ab9485454112442bbd7f2da8005792e5",
            "a9d8d8c5f2fb4bfaa663b6e2d4813697",
            "20a0969956054248a6d8fb9d23530374",
            "32f7988edc3d47e393ee8e466ae29771"
          ]
        },
        "id": "EEn3rqiJwa5n",
        "outputId": "c3dff60a-87df-4110-e6c0-354e30054774"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10178 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a69699b0d30349fc8c1255139b2fdb68"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# ===== Ultra-Fast QLoRA Config =====\n",
        "peft_config = LoraConfig(\n",
        "    r=8,  # Minimal rank\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.01,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Only most critical layers\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# ===== Speed-Optimized Training Args =====\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./quick_output\",\n",
        "    per_device_train_batch_size=4,  # Max batch size for T4\n",
        "    gradient_accumulation_steps=2,  # Effective batch=8\n",
        "    max_steps=150,                 # Super short run (~30 mins)\n",
        "    learning_rate=5e-4,            # Higher LR for fast learning\n",
        "    optim=\"adamw_8bit\",\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    remove_unused_columns=True,    # Saves memory\n",
        "    gradient_checkpointing=False,  # Disabled for speed (risky)\n",
        "    report_to=\"none\",\n",
        "    seed=3407,\n",
        ")\n",
        "\n",
        "# ===== Blazing Fast Trainer =====\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=256,  # Short sequences\n",
        "    packing=True,        # Critical for speed\n",
        "    dataset_num_proc=1,  # Less CPU overhead\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "433e5b75617e450cbffe5016f35ea4c4",
            "03e960d28a314b508d2641e20f837134",
            "0a375ccacab3429d82fb94d4d04ae5d1",
            "4e2df339b755498cb4fd7cee3d3295ee",
            "7db5ea275d4b495dab6a61124bd21aae",
            "23aa20b9b89447b186c320194f558ee4",
            "5f80908b7a2047f493f5be364af61e8e",
            "5edc5ae66ce0474d8e6553ec748a4848",
            "83fd4714ebab41dcaf667e405c083e32",
            "5758e2c3895c4b239a1f5ed7cf0eacae",
            "d5f6ffb467ff460ba360186a62372ca9"
          ]
        },
        "id": "M9fbtQFlyfdo",
        "outputId": "b99c66e6-859e-4797-ae40-9d669b519280"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/10178 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "433e5b75617e450cbffe5016f35ea4c4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51iHOI25zB48",
        "outputId": "09dcaebf-946a-4229-e29d-baf83367e588"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "11.402 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "abHZ5xRj3dSB",
        "outputId": "f8b87b35-8677-4cc0-9786-a96cb0848fdf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 10,178 | Num Epochs = 1 | Total steps = 150\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 1:09:05, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.279800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.976700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.860100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.862200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.835600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.829600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.799700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.786400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.794700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.799600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.783500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.775600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.806600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.745600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.790500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.791000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.806900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.777500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.781400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.779200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.784000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.762900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.788700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.781000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.767800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.754400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.763700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.789700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"List the prime numbers contained within the range.\", # instruction\n",
        "        \"1-50\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeV6lWWRIAp2",
        "outputId": "7268192b-1f76-42c4-c5c9-8e5d961d2a69"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nList the prime numbers contained within the range.\\n\\n### Input:\\n1-50\\n\\n### Response:\\nTo approach this task, let's analyze the request and break it down step by step.\\n\\n**Understanding the Request:**\\nThe task involves identifying the prime numbers contained within a given range. The range provided is 1-50, which means we need to find all prime numbers between 1 and 50.\\n\\n**Defining Prime Numbers:**\\nA prime number is a positive integer that is divisible only by itself and 1. For example, 2, 3, 5, 7, 11, and 13 are prime numbers because they are divisible only by themselves and 1.\\n\\n**Algorithm for Finding Prime Numbers:\"]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Convert these binary numbers to decimal.\", # instruction\n",
        "        \"1010, 1101, 1111\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bIupQEYICSv",
        "outputId": "94c8bc95-2d81-4d0e-dd90-4d1bf1dc13b3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert these binary numbers to decimal.\n",
            "\n",
            "### Input:\n",
            "1010, 1101, 1111\n",
            "\n",
            "### Response:\n",
            "To convert a binary number to decimal, you can use the following method: Start from the rightmost bit of the binary number, and assign each bit a weight based on its position. Then, sum up the weighted values of all the bits to get the decimal equivalent.\n",
            "\n",
            "Let's break down the process for each of the given binary numbers:\n",
            "\n",
            "**1010:**\n",
            "\n",
            "- Start from the rightmost bit (0), and give it a weight of 1 (2^0).\n",
            "- Next, move to the left and give the next bit (1) a weight of 2 (2^1).\n",
            "- Continue in this fashion, giving\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "8F2GV7tjIkd6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "# alpaca_prompt = You MUST run cells from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What is a famous tall tower in Paris?\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT4n-DgQIpnJ",
        "outputId": "5bc3fd1e-001e-43b9-ea4e-ec4ae0fd5346"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is a famous tall tower in Paris?\\n\\n### Input:\\n\\n\\n### Response:\\nTo approach this question, let's analyze the information provided and break it down step by step.\\n\\nUnderstanding the Question: The question asks for a famous tall tower in Paris. The context suggests that the tower is a well-known landmark in the city.\\n\\nIdentifying the Tower: To identify the tower, let's consider the characteristics\"]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "0dULRZeiItVd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ],
      "metadata": {
        "id": "ihS_vnOCIurD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ],
      "metadata": {
        "id": "XXu8L7j0IyuU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "uB578hRlJHrq",
        "outputId": "5bdf9ab4-c75f-4903-944e-6085ae07b450"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SFTConfig' object has no attribute 'save_as_json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-548571400.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Optional: Save the training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_as_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_model/training_args.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'SFTConfig' object has no attribute 'save_as_json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"You are an expert medical assistant named Aloe, developed by the High Performance Artificial Intelligence Group at Barcelona Supercomputing Center(BSC). You are to be a helpful, respectful, and honest assistant.', 'question': 'Answer the following question by selecting one of the possible choices. Explain the reasoning process of your decision\", # instruction\n",
        "        \"Given the following medical question with options, your task is to select the correct answer by the following process: First summarize what the question is about, then analyze each option individually, and finally select the correct answer through a step-by-step process and conclude by your final option selected. Question: A 16-year-old boy is brought to the physician by his mother because she is worried about his behavior. Yesterday, he was expelled from school for repeatedly skipping classes. Over the past 2 months, he was suspended 3 times for bullying and aggressive behavior towards his peers and teachers. Once, his neighbor found him smoking cigarettes in his backyard. In the past, he consistently maintained an A grade average and had been a regular attendee of youth group events at their local church. The mother first noticed this change in behavior 3 months ago, around the time at which his father moved out after discovering his wife was having an affair. Which of the following defense mechanisms best describes the change in this patient's behavior?Choices:A. Acting outB. ProjectionC. Passive aggressionD. Regression\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzW34wF6JdEh",
        "outputId": "7b5471db-6c0e-475e-aebe-ef5ded89cc8f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are an expert medical assistant named Aloe, developed by the High Performance Artificial Intelligence Group at Barcelona Supercomputing Center(BSC). You are to be a helpful, respectful, and honest assistant.', 'question': 'Answer the following question by selecting one of the possible choices. Explain the reasoning process of your decision\n",
            "\n",
            "### Input:\n",
            "Given the following medical question with options, your task is to select the correct answer by the following process: First summarize what the question is about, then analyze each option individually, and finally select the correct answer through a step-by-step process and conclude by your final option selected. Question: A 16-year-old boy is brought to the physician by his mother because she is worried about his behavior. Yesterday, he was expelled from school for repeatedly skipping classes. Over the past 2 months, he was suspended 3 times for bullying and aggressive behavior towards his peers and teachers. Once, his neighbor found him smoking cigarettes in his backyard. In the past, he consistently maintained an A grade average and had been a regular attendee of youth group events at their local church. The mother first noticed this change in behavior 3 months ago, around the time at which his father moved out after discovering his wife was having an affair. Which of the following defense mechanisms best describes the change in this patient's behavior?Choices:A. Acting outB. ProjectionC. Passive aggressionD. Regression\n",
            "\n",
            "### Response:\n",
            "To approach this question, let's analyze the information provided step by step.\n",
            "\n",
            "Patient's History and Behavior: The patient is a 16-year-old boy who has been experiencing a significant change in behavior over the past 3 months. His behavior has become increasingly disruptive and aggressive, leading to multiple suspensions from school and even an incident of smoking cigarettes. He was previously known for his good grades and positive involvement in community events.\n",
            "\n",
            "Recent Events and Changes: The patient's behavior began to change around the time his father moved out of the family home after discovering his wife's affair. This event, which represents a significant loss and a change in family\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "trainer.model.save_pretrained(\"output_model\")\n",
        "\n",
        "# Save the processing class (replacing deprecated tokenizer)\n",
        "trainer.processing_class.save_pretrained(\"output_model\")\n",
        "\n",
        "# Save the training arguments\n",
        "try:\n",
        "    # Attempt to access the embedded TrainingArguments (may vary by trl version)\n",
        "    training_args = trainer.args  # SFTConfig might wrap TrainingArguments differently\n",
        "    if hasattr(training_args, 'to_dict'):\n",
        "        import json\n",
        "        with open(\"output_model/training_args.json\", \"w\") as f:\n",
        "            json.dump(training_args.to_dict(), f, indent=4)\n",
        "    else:\n",
        "        # Fallback: Recreate TrainingArguments and save\n",
        "        from transformers import TrainingArguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"outputs\",\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=5,\n",
        "            num_train_epochs=4,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=not torch.cuda.is_bf16_supported(),\n",
        "            bf16=torch.cuda.is_bf16_supported(),\n",
        "            logging_steps=1,\n",
        "            optim=\"adamw_8bit\",\n",
        "            weight_decay=0.01,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            seed=3407,\n",
        "        )\n",
        "        training_args.save_to_json(\"output_model/training_args.json\")  # Correct method\n",
        "except AttributeError:\n",
        "    # Last resort: Save SFTConfig as JSON\n",
        "    import json\n",
        "    with open(\"output_model/sft_config.json\", \"w\") as f:\n",
        "        json.dump(trainer.args.to_dict(), f, indent=4)"
      ],
      "metadata": {
        "id": "C8uggZ3LKS1S"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}